{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Install Required Libraries\n"
      ],
      "metadata": {
        "id": "Uyxro0JnSwo0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olRmbh_9Srrx",
        "outputId": "00fe2227-e7e2-4ac8-ba90-af3b5f9922b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Install libraries (run this cell once, then you can skip it later)\n",
        "!pip install nltk spacy\n",
        "\n",
        "# Download spaCy English model\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports & Setup"
      ],
      "metadata": {
        "id": "5jxmIJ5bTAyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† Imports and basic setup\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import spacy\n",
        "\n",
        "# üîΩ Download NLTK resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# üîÅ Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ‚úÖ Prepare objects\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"Setup done ‚úÖ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKdZ6awgTFna",
        "outputId": "def4f6c2-1980-4768-db7e-bf970448f67a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup done ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample Text"
      ],
      "metadata": {
        "id": "hEfKMROgTIwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù Sample text (you can change this to any text you want)\n",
        "\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a field of Artificial Intelligence.\n",
        "It helps computers understand, interpret, and generate human language.\n",
        "Students are studying NLP to build chatbots, translators, and more!\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POLAyDgbTNp_",
        "outputId": "6ce0fd87-50fd-4120-e15e-e752972ef3dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "Natural Language Processing (NLP) is a field of Artificial Intelligence.\n",
            "It helps computers understand, interpret, and generate human language.\n",
            "Students are studying NLP to build chatbots, translators, and more!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Basic Cleaning Function"
      ],
      "metadata": {
        "id": "gzTLFmK3TQLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üßπ Step 1: Basic cleaning (lowercase, remove URLs, numbers, punctuation)\n",
        "\n",
        "def basic_cleaning(text: str) -> str:\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # 3. Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 4. Remove punctuation (keep only letters and spaces)\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "\n",
        "    # 5. Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "cleaned_text = basic_cleaning(text)\n",
        "print(\"Cleaned Text:\")\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETt9uVVdTUF1",
        "outputId": "2bc3928c-7806-415f-87db-a5934db31866"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text:\n",
            "natural language processing nlp is a field of artificial intelligence it helps computers understand interpret and generate human language students are studying nlp to build chatbots translators and more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "fLf90y3HTXLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÇÔ∏è Step 2: Tokenization (split into words)\n",
        "\n",
        "def tokenize(text: str):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "tokens = tokenize(cleaned_text)\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nNumber of tokens:\", len(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIIMOJLyTbSA",
        "outputId": "e20fe0b6-a272-4c5f-9617-6b65d4c95f98"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'it', 'helps', 'computers', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'students', 'are', 'studying', 'nlp', 'to', 'build', 'chatbots', 'translators', 'and', 'more']\n",
            "\n",
            "Number of tokens: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##topword Removal"
      ],
      "metadata": {
        "id": "pLAgJqL1TdrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üö´ Step 3: Remove stopwords (words like: is, the, and, of...)\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    filtered = [w for w in tokens if w not in stop_words]\n",
        "    return filtered\n",
        "\n",
        "tokens_no_sw = remove_stopwords(tokens)\n",
        "print(\"Tokens after stopword removal:\")\n",
        "print(tokens_no_sw)\n",
        "print(\"\\nNumber of tokens after stopword removal:\", len(tokens_no_sw))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjbxCaCXTgd2",
        "outputId": "59cad77e-c267-4355-d277-66de44682b0e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after stopword removal:\n",
            "['natural', 'language', 'processing', 'nlp', 'field', 'artificial', 'intelligence', 'helps', 'computers', 'understand', 'interpret', 'generate', 'human', 'language', 'students', 'studying', 'nlp', 'build', 'chatbots', 'translators']\n",
            "\n",
            "Number of tokens after stopword removal: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stemming"
      ],
      "metadata": {
        "id": "KUT_7WEKTi3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üå± Step 4: Stemming (cut word to its \"root\" form, sometimes ugly)\n",
        "\n",
        "def apply_stemming(tokens):\n",
        "    stemmed = [stemmer.stem(w) for w in tokens]\n",
        "    return stemmed\n",
        "\n",
        "stemmed_tokens = apply_stemming(tokens_no_sw)\n",
        "print(\"Stemmed tokens:\")\n",
        "print(stemmed_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlUvlAkaTnFF",
        "outputId": "5f705f5d-c8c6-442f-d4b0-d1c6478293c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed tokens:\n",
            "['natur', 'languag', 'process', 'nlp', 'field', 'artifici', 'intellig', 'help', 'comput', 'understand', 'interpret', 'gener', 'human', 'languag', 'student', 'studi', 'nlp', 'build', 'chatbot', 'translat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lemmatization (NLTK + spaCy)"
      ],
      "metadata": {
        "id": "yp4ukQFvTpTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üå≥ Step 5: Lemmatization (get proper dictionary form of words)\n",
        "\n",
        "# Option A: NLTK lemmatizer\n",
        "def apply_lemmatization_nltk(tokens):\n",
        "    lemmas = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "    return lemmas\n",
        "\n",
        "lemmatized_tokens_nltk = apply_lemmatization_nltk(tokens_no_sw)\n",
        "print(\"Lemmatized tokens (NLTK):\")\n",
        "print(lemmatized_tokens_nltk)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36-4b48jTvse",
        "outputId": "377aa323-5520-43f2-d4fa-fd9973dd6a2a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens (NLTK):\n",
            "['natural', 'language', 'processing', 'nlp', 'field', 'artificial', 'intelligence', 'help', 'computer', 'understand', 'interpret', 'generate', 'human', 'language', 'student', 'studying', 'nlp', 'build', 'chatbots', 'translator']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Full Pipeline Function"
      ],
      "metadata": {
        "id": "PSjTtK-TT02A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîó Full preprocessing pipeline: from raw text to all stages\n",
        "\n",
        "def preprocess_pipeline(raw_text: str):\n",
        "    print(\"üîπ Original Text:\")\n",
        "    print(raw_text)\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    cleaned = basic_cleaning(raw_text)\n",
        "    print(\"üîπ After Cleaning:\")\n",
        "    print(cleaned)\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    tokens = tokenize(cleaned)\n",
        "    print(\"üîπ Tokens:\")\n",
        "    print(tokens)\n",
        "    print(\"Number of tokens:\", len(tokens))\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    tokens_no_sw = remove_stopwords(tokens)\n",
        "    print(\"üîπ After Stopword Removal:\")\n",
        "    print(tokens_no_sw)\n",
        "    print(\"Number of tokens (no stopwords):\", len(tokens_no_sw))\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    stemmed = apply_stemming(tokens_no_sw)\n",
        "    print(\"üîπ After Stemming:\")\n",
        "    print(stemmed)\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    lemmas_nltk = apply_lemmatization_nltk(tokens_no_sw)\n",
        "    print(\"üîπ After Lemmatization (NLTK):\")\n",
        "    print(lemmas_nltk)\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Return data if you want to use it later\n",
        "    return {\n",
        "        \"cleaned\": cleaned,\n",
        "        \"tokens\": tokens,\n",
        "        \"tokens_no_sw\": tokens_no_sw,\n",
        "        \"stemmed\": stemmed,\n",
        "        \"lemmas_nltk\": lemmas_nltk\n",
        "    }\n",
        "\n",
        "# ‚ñ∂Ô∏è Run the pipeline on our sample text\n",
        "results = preprocess_pipeline(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1UGkc2VT0VF",
        "outputId": "aa7f808e-6783-444c-95a8-cf7f63788fa7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Original Text:\n",
            "\n",
            "Natural Language Processing (NLP) is a field of Artificial Intelligence.\n",
            "It helps computers understand, interpret, and generate human language.\n",
            "Students are studying NLP to build chatbots, translators, and more!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ After Cleaning:\n",
            "natural language processing nlp is a field of artificial intelligence it helps computers understand interpret and generate human language students are studying nlp to build chatbots translators and more\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ Tokens:\n",
            "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'it', 'helps', 'computers', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'students', 'are', 'studying', 'nlp', 'to', 'build', 'chatbots', 'translators', 'and', 'more']\n",
            "Number of tokens: 29\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ After Stopword Removal:\n",
            "['natural', 'language', 'processing', 'nlp', 'field', 'artificial', 'intelligence', 'helps', 'computers', 'understand', 'interpret', 'generate', 'human', 'language', 'students', 'studying', 'nlp', 'build', 'chatbots', 'translators']\n",
            "Number of tokens (no stopwords): 20\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ After Stemming:\n",
            "['natur', 'languag', 'process', 'nlp', 'field', 'artifici', 'intellig', 'help', 'comput', 'understand', 'interpret', 'gener', 'human', 'languag', 'student', 'studi', 'nlp', 'build', 'chatbot', 'translat']\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ After Lemmatization (NLTK):\n",
            "['natural', 'language', 'processing', 'nlp', 'field', 'artificial', 'intelligence', 'help', 'computer', 'understand', 'interpret', 'generate', 'human', 'language', 'student', 'studying', 'nlp', 'build', 'chatbots', 'translator']\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Try Your Own Text"
      ],
      "metadata": {
        "id": "hMV7NbVhT7TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úçÔ∏è Test with your own sentence or paragraph\n",
        "\n",
        "my_text = input(\"Write any English sentence or paragraph:\\n\")\n",
        "\n",
        "_ = preprocess_pipeline(my_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMAA0yWeUAnS",
        "outputId": "2c2262f5-ea70-4ff4-e431-3c8e7a4bd696"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write any English sentence or paragraph:\n",
            "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
            "üîπ Original Text:\n",
            "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ After Cleaning:\n",
            "nltk is a leading platform for building python programs to work with human language data it provides easy to use interfaces to over corpora and lexical resources such as wordnet along with a suite of text processing libraries for classification tokenization stemming tagging parsing and semantic reasoning wrappers for industrial strength nlp libraries and an active discussion forum\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ Tokens:\n",
            "['nltk', 'is', 'a', 'leading', 'platform', 'for', 'building', 'python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', 'it', 'provides', 'easy', 'to', 'use', 'interfaces', 'to', 'over', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'wordnet', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', 'tokenization', 'stemming', 'tagging', 'parsing', 'and', 'semantic', 'reasoning', 'wrappers', 'for', 'industrial', 'strength', 'nlp', 'libraries', 'and', 'an', 'active', 'discussion', 'forum']\n",
            "Number of tokens: 58\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ After Stopword Removal:\n",
            "['nltk', 'leading', 'platform', 'building', 'python', 'programs', 'work', 'human', 'language', 'data', 'provides', 'easy', 'use', 'interfaces', 'corpora', 'lexical', 'resources', 'wordnet', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', 'tokenization', 'stemming', 'tagging', 'parsing', 'semantic', 'reasoning', 'wrappers', 'industrial', 'strength', 'nlp', 'libraries', 'active', 'discussion', 'forum']\n",
            "Number of tokens (no stopwords): 38\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ After Stemming:\n",
            "['nltk', 'lead', 'platform', 'build', 'python', 'program', 'work', 'human', 'languag', 'data', 'provid', 'easi', 'use', 'interfac', 'corpora', 'lexic', 'resourc', 'wordnet', 'along', 'suit', 'text', 'process', 'librari', 'classif', 'token', 'stem', 'tag', 'pars', 'semant', 'reason', 'wrapper', 'industri', 'strength', 'nlp', 'librari', 'activ', 'discuss', 'forum']\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ After Lemmatization (NLTK):\n",
            "['nltk', 'leading', 'platform', 'building', 'python', 'program', 'work', 'human', 'language', 'data', 'provides', 'easy', 'use', 'interface', 'corpus', 'lexical', 'resource', 'wordnet', 'along', 'suite', 'text', 'processing', 'library', 'classification', 'tokenization', 'stemming', 'tagging', 'parsing', 'semantic', 'reasoning', 'wrapper', 'industrial', 'strength', 'nlp', 'library', 'active', 'discussion', 'forum']\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}